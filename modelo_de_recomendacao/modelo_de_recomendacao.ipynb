{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo - Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas Necessárias para o modelo de recomendação\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV \n",
    "from surprise import accuracy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo a leitura e carregamento dos dados para o formato da lib Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "#Base com avaliações absolutas com todas as avaliações\n",
    "data = Dataset.load_from_df(df_recom[['user_id', 'product_id', 'rating']], reader)\n",
    "#Base com avaliações normalizadas com todas as avaliações\n",
    "data_normalized = Dataset.load_from_df(df_recom[['user_id', 'product_id', 'normalized_rating']], reader)\n",
    "#Base com avaliações normalizadas com apenas com usuários que avaliaram mais de um vez\n",
    "data_normalized_1 = Dataset.load_from_df(df_recom_reviw_1[['user_id', 'product_id', 'normalized_rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px\">**Configurando Melhores Carecterísticas do Modelo**</span>\n",
    "\n",
    "A biblioteca Surprise tem uma função bastante interessante que podemos selecionar hiper parâmetros que auxiliam a encontrar a melhor configuração para o modelo de configuração através da função **GridSearchCV**. Os hiper parâmetros que podem ser definidos:\n",
    "\n",
    "<span style=\"font-size:20px\">**n_factors**</span>\n",
    "\n",
    "Trata sobre a consideração de carecrísticas não vísiveis dos usuários que não estão no modelo.\n",
    "- Entre 10 à 30 características ocultas é aplicado para modelos mais simples e diminuindo a chance de overfitting do modelo.\n",
    "- Acima de 100 é aplicado para modelo mais complexos e pode aumentar a possibildiade overfitting para bases de dados pequenas.\n",
    "\n",
    "<span style=\"font-size:20px\">**lr_all**</span>\n",
    "\n",
    "Taxa de aprendizado do modelo, controla o tamanho dos passos que o modelo segue durante o treino. Esse parâmetro é o que \"ajusta\" os pesos a cada iteração.\n",
    "- Aplicando um valor baixo, como 0.002, o treino do modelo é mais lento, porém é considerado mais estável.\n",
    "- Aplicando um valor alto, como 0.01, o treino do modelo é mais rápido, porém pode oscilar.\n",
    "\n",
    "<span style=\"font-size:20px\">**reg_all**</span>\n",
    "\n",
    "Esse parâmetro é o que \"penaliza\", em maior ou menor peso, a previsão de dados mais distoantes. Esse parâmetro auxília para evitar o overfitting do modelo.\n",
    "- Aplicando um valor baixo, como 0.02, há pouca penalização, aumento a possibilidade de overfitting.\n",
    "- Aplicando um valor alto, como 0.01, há mais penalização, podendo substimar relações reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Apenas usuários com pelo menos 5 avaliações?\n",
    "Não vale a pena, porque 97% da base são de usuários que fizeram um pedido.\n",
    "# Qtd Média de avaliação por usuário\n",
    "Próximo de 1 por causa base.\n",
    "# Normalizar notas por usuário?\n",
    "Próximo de 1 por causa base.\n",
    "\n",
    "Modelos como SVD, SVD++, e BaselineOnly funcionam melhor quando os dados foram centrados em torno de médias.\n",
    "\n",
    "O SVD, por exemplo, decompõe a matriz de interações — se as notas estiverem normalizadas, a decomposição converge melhor e capta padrões reais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Base com avaliações absolutas com todas as avaliações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   39.8s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=1)]: Done 364 tasks      | elapsed:  8.7min\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [50, 100, 125, 150],\n",
    "    'lr_all': [0.005, 0.008, 0.009, 0.01, 0.02],\n",
    "    'reg_all': [0.05, 0.075, 0.1, 0.15, 0.2]\n",
    "}\n",
    "gs = GridSearchCV(SVD, #Modelo\n",
    "                  param_grid, #Hiper parâmetros\n",
    "                  measures=['rmse', 'mae'], #Métricas de Avaliação\n",
    "                  cv=5, #Qtd de recortes na bases de dados \n",
    "                  joblib_verbose=2) # Controle do nível de verbosidade\n",
    "gs.fit(data) #Treino para encontrar os melhos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.05}\n",
      "RMSE Score: nan\n",
      "\n",
      "MAE {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.05}\n",
      "MAE Score: nan\n"
     ]
    }
   ],
   "source": [
    "print('RMSE', gs.best_params['rmse'])\n",
    "print(f\"RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "print('\\nMAE', gs.best_params['mae'])\n",
    "print(f\"MAE Score: {gs.best_score['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base com avaliações normalizadas com todas as avaliações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done 364 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=1)]: Done 647 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=1)]: Done 1012 tasks      | elapsed: 12.1min\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [1, 2, 5, 10, 50, 100],\n",
    "    'lr_all': [0.0005, 0.001, 0.0015, 0.002, 0.005, 0.01],\n",
    "    'reg_all': [0.01, 0.015, 0.02, 0.025, 0.05, 0.1]\n",
    "}\n",
    "gs = GridSearchCV(SVD, #Modelo\n",
    "                  param_grid, #Hiper parâmetros\n",
    "                  measures=['rmse', 'mae'], #Métricas de Avaliação\n",
    "                  cv=5, #Qtd de recortes na bases de dados \n",
    "                  joblib_verbose=2) # Controle do nível de verbosidade\n",
    "gs.fit(data_normalized) #Treino para encontrar os melhos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE {'n_factors': 1, 'lr_all': 0.0005, 'reg_all': 0.01}\n",
      "RMSE Score: nan\n",
      "\n",
      "MAE {'n_factors': 1, 'lr_all': 0.0005, 'reg_all': 0.01}\n",
      "MAE Score: nan\n"
     ]
    }
   ],
   "source": [
    "print('RMSE', gs.best_params['rmse'])\n",
    "print(f\"RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "print('\\nMAE', gs.best_params['mae'])\n",
    "print(f\"MAE Score: {gs.best_score['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=1)]: Done 364 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=1)]: Done 647 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=1)]: Done 1012 tasks      | elapsed:   35.0s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [1, 2, 5, 10, 50, 100],\n",
    "    'lr_all': [0.0005, 0.001, 0.0015, 0.002, 0.005, 0.01],\n",
    "    'reg_all': [0.01, 0.015, 0.02, 0.025, 0.05, 0.1]\n",
    "}\n",
    "gs = GridSearchCV(SVD, #Modelo\n",
    "                  param_grid, #Hiper parâmetros\n",
    "                  measures=['rmse', 'mae'], #Métricas de Avaliação\n",
    "                  cv=5, #Qtd de recortes na bases de dados \n",
    "                  joblib_verbose=2) # Controle do nível de verbosidade\n",
    "gs.fit(data_normalized_1) #Treino para encontrar os melhos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE {'n_factors': 1, 'lr_all': 0.0005, 'reg_all': 0.01}\n",
      "RMSE Score: nan\n",
      "\n",
      "MAE {'n_factors': 1, 'lr_all': 0.0005, 'reg_all': 0.01}\n",
      "MAE Score: nan\n"
     ]
    }
   ],
   "source": [
    "print('RMSE', gs.best_params['rmse'])\n",
    "print(f\"RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "print('\\nMAE', gs.best_params['mae'])\n",
    "print(f\"MAE Score: {gs.best_score['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Após realizar o treino com vários valores para parâmetros, chegou o momento de verificar, com base no resultado do RMSE e o MAE, quais valores dos hiper parâmetros são os mais ideais para gerar um modelo de recomendação de maior qualidade de previsão.\n",
    "\n",
    "Antes de verificar os melhores valores para os hiper parâmetros, vamos relembrar o que significa o RMSE e o MAE que vamos usar como base para essa escolha:\n",
    "\n",
    "<span style=\"font-size:20px\">**Root Mean Squared Error (RMSE)**</span>\n",
    "\n",
    "Essa métrica mede o erro médio ao quadrado entre o valor previsto e o valor real, auxilia no entendimento de quando o modelo pode cometer erros mais altos de previsão.\n",
    "\n",
    "<span style=\"font-size:20px\">**Mean Absolute Error (MAE)**</span>\n",
    "\n",
    "Essa métrica mede a diferença absoluta média entre o valor previsto e o valor real, auxilia no entendimento da qualidade do modelo em prever os valores reais.\n",
    "\n",
    "\n",
    "**Interpretação**\n",
    "\n",
    "Para esse caso, que temos notas de produtos entre 1 e 5, poderiamos interpretar o RMSE e o MAE da seguinte forma:\n",
    "\n",
    "Abaixo 0.7 - Excelente | Entre 0.7 e 0.9 - Bom | Entre 0.9 e 1.1 - Ok | Acima de 1.1 - Ruim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino, Teste e Validação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: nan\n",
      "RMSE no teste: nan\n",
      "MAE:  nan\n",
      "MAE no teste: nan\n"
     ]
    }
   ],
   "source": [
    "# Divisão dos dados\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Modelo com os melhores parâmetros\n",
    "best_model = gs.best_estimator['rmse']\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "predictions = best_model.test(testset)\n",
    "print(\"RMSE no teste:\", accuracy.rmse(predictions))\n",
    "print(\"MAE no teste:\", accuracy.mae(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1_at_k(predictions, k=3, threshold=4.0):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    for est_true in user_est_true.values():\n",
    "        est_true.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_k = est_true[:k]\n",
    "        tp = sum((true_r >= threshold) for _, true_r in top_k)\n",
    "        fp = k - tp\n",
    "        fn = sum((true_r >= threshold) for _, true_r in est_true[k:])\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'Precision': sum(precisions) / len(precisions),\n",
    "        'Recall': sum(recalls) / len(recalls),\n",
    "        'F1': sum(f1s) / len(f1s)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas Top-N: {'Precision': 0.265147721582319, 'Recall': 0.7871877260334947, 'F1': 0.395999554887888}\n"
     ]
    }
   ],
   "source": [
    "metrics = precision_recall_f1_at_k(predictions, k=3)\n",
    "print(\"Métricas Top-N:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com todos os dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1bdbf123c50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinar com 100% dos dados\n",
    "trainset_full = data.build_full_trainset()\n",
    "best_model.fit(trainset_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recomendações para o usuário 345ecd01c38d18a9036ed96c73b8d066:\n",
      "Item 0 - Nota prevista: 5.00\n",
      "Item 1 - Nota prevista: 5.00\n",
      "Item 2 - Nota prevista: 5.00\n"
     ]
    }
   ],
   "source": [
    "# Treinar com 100% dos dados\n",
    "trainset_full = data.build_full_trainset()\n",
    "best_model.fit(trainset_full)\n",
    "\n",
    "# Recomendação para usuário 1: itens que ele ainda não avaliou\n",
    "user_id = \"345ecd01c38d18a9036ed96c73b8d066\"\n",
    "all_items = set(iid for (_, iid, _) in trainset_full.all_ratings())\n",
    "rated_items = set(j for (j, _) in trainset_full.ur[trainset_full.to_inner_uid(user_id)])\n",
    "unseen_items = all_items - rated_items\n",
    "\n",
    "# Predizer e ordenar por melhor nota prevista\n",
    "recommendations = [\n",
    "    (iid, best_model.predict(user_id, iid).est) for iid in unseen_items\n",
    "]\n",
    "recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Top 3 recomendações\n",
    "print(f\"Top 3 recomendações para o usuário {user_id}:\")\n",
    "for iid, score in recommendations[:3]:\n",
    "    print(f\"Item {iid} - Nota prevista: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Exemplo: Recomendação para o usuário 1\u001b[39;00m\n\u001b[0;32m     23\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 24\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m recomendar_produtos(user_id, \u001b[43mdf\u001b[49m, model)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecomendações para o usuário \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product, rating \u001b[38;5;129;01min\u001b[39;00m recommendations:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Função para recomendar produtos a um usuário específico\n",
    "def recomendar_produtos(user_id, df, model, n_recommendations=3):\n",
    "    \"\"\"Gera recomendações de produtos para um usuário.\"\"\"\n",
    "    \n",
    "    # Obtém todos os produtos únicos\n",
    "    all_products = df['product_id'].unique()\n",
    "    \n",
    "    # Obtém produtos já avaliados pelo usuário\n",
    "    rated_products = df[df['user_id'] == user_id]['product_id'].values\n",
    "    \n",
    "    # Filtra apenas os produtos que o usuário ainda não avaliou\n",
    "    products_to_predict = [p for p in all_products if p not in rated_products]\n",
    "    \n",
    "    # Faz previsões para esses produtos\n",
    "    predictions = [(p, model.predict(user_id, p).est) for p in products_to_predict]\n",
    "    \n",
    "    # Ordena por nota prevista (maior para menor)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return predictions[:n_recommendations]\n",
    "\n",
    "# Exemplo: Recomendação para o usuário 1\n",
    "user_id = 1\n",
    "recommendations = recomendar_produtos(user_id, df, model)\n",
    "\n",
    "print(f\"\\nRecomendações para o usuário {user_id}:\")\n",
    "for product, rating in recommendations:\n",
    "    print(f\"Produto {product} - Nota prevista: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendação de Produtos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separação dos dados para recomendação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultima_compra = dataframe.groupby(\"customer_unique_id\").agg({'order_purchase_timestamp':max})\n",
    "df_ultima_compra.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultima_compra['order_purchase_timestamp'] = pd.to_datetime(df_ultima_compra['order_purchase_timestamp'])\n",
    "data_referencia = pd.to_datetime('2018-09-10')\n",
    "df_ultima_compra['dias_desde_da_ultima_compra'] = (data_referencia - df_ultima_compra['order_purchase_timestamp']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30_90_dias = df_ultima_compra[(df_ultima_compra['dias_desde_da_ultima_compra'] >= 30) & (df_ultima_compra['dias_desde_da_ultima_compra'] <= 90)]['customer_unique_id']\n",
    "df_30_90_dias = df_30_90_dias.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
